{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SR-SAN \n",
    "This is the code for the paper [Session-based Recommendation with Self-Attention Networks](https://arxiv.org/abs/2102.01922) by Jun Fang. \n",
    "\n",
    "I have taken the original code and converted it into a Jupytr notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries that needed to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "from utils import  Data, split_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the Self-Attention Network which is implemented as a [Transform Encoder Layer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html). I have added additional comments to the module below to highlight the various important components as described in the paper. The rest of the code are fairly standard components of a Pytorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionNetwork(Module):\n",
    "    def __init__(self, opt, n_node):\n",
    "        super(SelfAttentionNetwork, self).__init__()\n",
    "        self.hidden_size = opt.hiddenSize\n",
    "        self.n_node = n_node\n",
    "        self.batch_size = opt.batchSize\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_node, self.hidden_size)  # This is the embedding layer.\n",
    "        self.transformerEncoderLayer = TransformerEncoderLayer(d_model=self.hidden_size, nhead=opt.nhead,dim_feedforward=self.hidden_size * opt.feedforward)  # The transformer encoder layer\n",
    "        self.transformerEncoder = TransformerEncoder(self.transformerEncoderLayer, opt.layer)  # The transformer encoder, composed of multiple layers. \n",
    "        self.loss_function = nn.CrossEntropyLoss()  # This defines the loss function.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def compute_scores(self, hidden, mask):\n",
    "        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size\n",
    "        b = self.embedding.weight[1:]  # n_nodes x latent_size\n",
    "        scores = torch.matmul(ht, b.transpose(1, 0))\n",
    "        return scores\n",
    "\n",
    "    def forward(self, inputs, A):\n",
    "        hidden = self.embedding(inputs)\n",
    "        hidden = hidden.transpose(0,1).contiguous()\n",
    "        hidden = self.transformerEncoder(hidden)\n",
    "        hidden = hidden.transpose(0,1).contiguous()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function transforms the Tensor into a Pytorch CUDA object, if CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_to_cuda(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cuda()\n",
    "    else:\n",
    "        return variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function transforms the Tensor into a Pytorch CPU object, if CUDA object is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_to_cpu(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cpu()\n",
    "    else:\n",
    "        return variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This forward function computes the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, i, data):\n",
    "    alias_inputs, A, items, mask, targets = data.get_slice(i)\n",
    "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
    "    items = trans_to_cuda(torch.Tensor(items).long())\n",
    "    A = trans_to_cuda(torch.Tensor(A).float())\n",
    "    mask = trans_to_cuda(torch.Tensor(mask).long())\n",
    "    hidden = model(items, A)\n",
    "    get = lambda i: hidden[i][alias_inputs[i]]\n",
    "    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n",
    "    return targets, model.compute_scores(seq_hidden, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test` function is used to train the model. First, the function uses an optimizer to find the minimum loss of the model. After that, it calculates MRR@20 and HR@20 values and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, train_data, test_data):    \n",
    "    print('start training: ', datetime.datetime.now())\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    slices = train_data.generate_batch(model.batch_size)\n",
    "    for i, j in zip(slices, np.arange(len(slices))):\n",
    "        model.optimizer.zero_grad()\n",
    "        targets, scores = forward(model, i, train_data)\n",
    "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
    "        loss = model.loss_function(scores, targets - 1)\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        total_loss += loss\n",
    "        if j % int(len(slices) / 5 + 1) == 0:\n",
    "            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n",
    "    print('\\tLoss:\\t%.3f' % total_loss)\n",
    "\n",
    "    print('start predicting: ', datetime.datetime.now())\n",
    "    model.eval()\n",
    "    hit, mrr = [], []\n",
    "    slices = test_data.generate_batch(model.batch_size)\n",
    "    for i in slices:\n",
    "        targets, scores = forward(model, i, test_data)\n",
    "        sub_scores = scores.topk(20)[1]\n",
    "        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n",
    "        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n",
    "            hit.append(np.isin(target - 1, score))\n",
    "            if len(np.where(score == target - 1)[0]) == 0:\n",
    "                mrr.append(0)\n",
    "            else:\n",
    "                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
    "    hit = np.mean(hit) * 100\n",
    "    mrr = np.mean(mrr) * 100\n",
    "    model.scheduler.step()\n",
    "    return hit, mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the options (hyperparameters) used in the code. I have preserved the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option:\n",
    "    dataset = \"yoochoose1_64\" # dataset name: diginetica/yoochoose1_64\n",
    "    valid_portion = 0.1       # split the portion of training set as validation set\n",
    "    epoch = 12                # the number of epochs to train for\n",
    "    validation = False\n",
    "    hiddenSize = 96           # hidden state size\n",
    "    batchSize = 100           # input batch size\n",
    "    nhead = 2                 # the number of heads of multi-head attention\n",
    "    feedforward = 4           # the multipler of hidden state size\n",
    "    layer = 1                 # number of SAN layers\n",
    "    lr = 0.001                # learning rate\n",
    "    l2 = 1e-5                 # l2 penalty\n",
    "    lr_dc_step = 3            # the number of steps after which the learning rate decay\n",
    "    lr_dc = 0.1               # learning rate decay rate\n",
    "    patience = 10             # the number of epoch to wait before early stop\n",
    "\n",
    "opt = Option()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will load the train and test pre-processed code, which is encoded as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pickle.load(open('./datasets/' + opt.dataset + '/train.txt', 'rb'))\n",
    "if opt.validation:\n",
    "    train_data, valid_data = split_validation(train_data, opt.valid_portion)\n",
    "    test_data = valid_data\n",
    "else:\n",
    "    test_data = pickle.load(open('./datasets/' + opt.dataset + '/test.txt', 'rb'))\n",
    "\n",
    "train_data = Data(train_data, shuffle=True)\n",
    "test_data = Data(test_data, shuffle=False)\n",
    "\n",
    "# The number of nodes based on the dataset.\n",
    "if opt.dataset == 'diginetica':\n",
    "    n_node = 43098\n",
    "else:\n",
    "    n_node = 37484"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the SAN model is created. If CUDA is enabled, it will use the CUDA version of the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trans_to_cuda(SelfAttentionNetwork(opt, n_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will train the model. The best HR@20 and MRR@20 determined in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "epoch:  0\n",
      "start training:  2022-05-01 18:31:22.966508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chemi\\source\\tcss556\\.venv\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/3699] Loss: 10.5344\n",
      "[740/3699] Loss: 6.2977\n",
      "[1480/3699] Loss: 5.7345\n",
      "[2220/3699] Loss: 4.5850\n",
      "[2960/3699] Loss: 4.8966\n",
      "\tLoss:\t20159.918\n",
      "start predicting:  2022-05-01 18:49:23.135382\n",
      "Best Result:\n",
      "\tRecall@20:\t69.2798\tMMR@20:\t29.2985\tEpoch:\t0,\t0\n",
      "-------------------------------------------------------\n",
      "epoch:  1\n",
      "start training:  2022-05-01 18:49:57.106424\n",
      "[0/3699] Loss: 4.1349\n",
      "[740/3699] Loss: 4.3210\n",
      "[1480/3699] Loss: 4.3392\n",
      "[2220/3699] Loss: 4.2475\n",
      "[2960/3699] Loss: 4.1755\n",
      "\tLoss:\t15730.980\n",
      "start predicting:  2022-05-01 19:08:40.334549\n",
      "Best Result:\n",
      "\tRecall@20:\t70.3621\tMMR@20:\t29.4718\tEpoch:\t1,\t1\n",
      "-------------------------------------------------------\n",
      "epoch:  2\n",
      "start training:  2022-05-01 19:09:12.521523\n",
      "[0/3699] Loss: 4.1631\n",
      "[740/3699] Loss: 3.8239\n",
      "[1480/3699] Loss: 4.3064\n",
      "[2220/3699] Loss: 4.2536\n",
      "[2960/3699] Loss: 4.0381\n",
      "\tLoss:\t14961.311\n",
      "start predicting:  2022-05-01 19:27:40.090421\n",
      "Best Result:\n",
      "\tRecall@20:\t70.4462\tMMR@20:\t29.9591\tEpoch:\t2,\t2\n",
      "-------------------------------------------------------\n",
      "epoch:  3\n",
      "start training:  2022-05-01 19:28:13.246991\n",
      "[0/3699] Loss: 3.8430\n",
      "[740/3699] Loss: 3.3618\n",
      "[1480/3699] Loss: 4.0760\n",
      "[2220/3699] Loss: 4.0940\n",
      "[2960/3699] Loss: 4.1837\n",
      "\tLoss:\t13757.634\n",
      "start predicting:  2022-05-01 19:46:55.000050\n",
      "Best Result:\n",
      "\tRecall@20:\t71.4301\tMMR@20:\t31.1167\tEpoch:\t3,\t3\n",
      "-------------------------------------------------------\n",
      "epoch:  4\n",
      "start training:  2022-05-01 19:47:28.530158\n",
      "[0/3699] Loss: 3.8532\n",
      "[740/3699] Loss: 3.5020\n",
      "[1480/3699] Loss: 3.8389\n",
      "[2220/3699] Loss: 3.5329\n",
      "[2960/3699] Loss: 3.5741\n",
      "\tLoss:\t13532.252\n",
      "start predicting:  2022-05-01 20:06:10.182941\n",
      "Best Result:\n",
      "\tRecall@20:\t71.4677\tMMR@20:\t31.2668\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "epoch:  5\n",
      "start training:  2022-05-01 20:06:43.635211\n",
      "[0/3699] Loss: 3.7574\n",
      "[740/3699] Loss: 3.9580\n",
      "[1480/3699] Loss: 4.1654\n",
      "[2220/3699] Loss: 3.5332\n",
      "[2960/3699] Loss: 3.9157\n",
      "\tLoss:\t13418.293\n",
      "start predicting:  2022-05-01 20:25:12.568073\n",
      "Best Result:\n",
      "\tRecall@20:\t71.4677\tMMR@20:\t31.2977\tEpoch:\t4,\t5\n",
      "-------------------------------------------------------\n",
      "epoch:  6\n",
      "start training:  2022-05-01 20:25:45.057116\n",
      "[0/3699] Loss: 3.8518\n",
      "[740/3699] Loss: 3.5396\n",
      "[1480/3699] Loss: 3.3636\n",
      "[2220/3699] Loss: 3.3773\n",
      "[2960/3699] Loss: 3.6041\n",
      "\tLoss:\t13209.213\n",
      "start predicting:  2022-05-01 20:44:07.727515\n",
      "Best Result:\n",
      "\tRecall@20:\t71.4677\tMMR@20:\t31.3181\tEpoch:\t4,\t6\n",
      "-------------------------------------------------------\n",
      "epoch:  7\n",
      "start training:  2022-05-01 20:44:40.161954\n",
      "[0/3699] Loss: 3.5777\n",
      "[740/3699] Loss: 3.4314\n",
      "[1480/3699] Loss: 3.5054\n",
      "[2220/3699] Loss: 3.9273\n",
      "[2960/3699] Loss: 3.2726\n",
      "\tLoss:\t13193.502\n",
      "start predicting:  2022-05-01 21:02:55.914119\n",
      "Best Result:\n",
      "\tRecall@20:\t71.4677\tMMR@20:\t31.3197\tEpoch:\t4,\t7\n",
      "-------------------------------------------------------\n",
      "epoch:  8\n",
      "start training:  2022-05-01 21:03:28.028628\n",
      "[0/3699] Loss: 3.6709\n",
      "[740/3699] Loss: 3.3486\n",
      "[1480/3699] Loss: 3.5099\n",
      "[2220/3699] Loss: 3.9882\n",
      "[2960/3699] Loss: 3.4249\n",
      "\tLoss:\t13180.891\n",
      "start predicting:  2022-05-01 21:21:33.488343\n",
      "Best Result:\n",
      "\tRecall@20:\t71.4677\tMMR@20:\t31.3197\tEpoch:\t4,\t7\n",
      "-------------------------------------------------------\n",
      "epoch:  9\n",
      "start training:  2022-05-01 21:22:05.912887\n",
      "[0/3699] Loss: 3.6580\n",
      "[740/3699] Loss: 3.2295\n",
      "[1480/3699] Loss: 3.1369\n",
      "[2220/3699] Loss: 4.0968\n",
      "[2960/3699] Loss: 3.7701\n",
      "\tLoss:\t13159.753\n",
      "start predicting:  2022-05-01 21:40:14.730027\n",
      "Best Result:\n",
      "\tRecall@20:\t71.4677\tMMR@20:\t31.3197\tEpoch:\t4,\t7\n",
      "-------------------------------------------------------\n",
      "epoch:  10\n",
      "start training:  2022-05-01 21:40:46.597491\n",
      "[0/3699] Loss: 3.8565\n",
      "[740/3699] Loss: 3.8482\n",
      "[1480/3699] Loss: 3.4427\n",
      "[2220/3699] Loss: 3.8174\n",
      "[2960/3699] Loss: 3.4376\n",
      "\tLoss:\t13159.145\n",
      "start predicting:  2022-05-01 21:59:02.066026\n",
      "Best Result:\n",
      "\tRecall@20:\t71.4677\tMMR@20:\t31.3229\tEpoch:\t4,\t10\n",
      "-------------------------------------------------------\n",
      "epoch:  11\n",
      "start training:  2022-05-01 21:59:34.204804\n",
      "[0/3699] Loss: 3.3846\n",
      "[740/3699] Loss: 3.6706\n",
      "[1480/3699] Loss: 3.5463\n",
      "[2220/3699] Loss: 3.4053\n",
      "[2960/3699] Loss: 3.8224\n",
      "\tLoss:\t13153.291\n",
      "start predicting:  2022-05-01 22:17:42.714917\n",
      "Best Result:\n",
      "\tRecall@20:\t71.4677\tMMR@20:\t31.3229\tEpoch:\t4,\t10\n",
      "-------------------------------------------------------\n",
      "Run time: 13612.302420 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "best_result = [0, 0]\n",
    "best_epoch = [0, 0]\n",
    "bad_counter = 0\n",
    "for epoch in range(opt.epoch):\n",
    "    print('-------------------------------------------------------')\n",
    "    print('epoch: ', epoch)\n",
    "    hit, mrr = train_test(model, train_data, test_data)\n",
    "    flag = 0\n",
    "    if hit >= best_result[0]:\n",
    "        best_result[0] = hit\n",
    "        best_epoch[0] = epoch\n",
    "        flag = 1\n",
    "    if mrr >= best_result[1]:\n",
    "        best_result[1] = mrr\n",
    "        best_epoch[1] = epoch\n",
    "        flag = 1\n",
    "    print('Best Result:')\n",
    "    print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
    "    bad_counter += 1 - flag\n",
    "    if bad_counter >= opt.patience:\n",
    "        break\n",
    "print('-------------------------------------------------------')\n",
    "end = time.time()\n",
    "print(\"Run time: %f s\" % (end - start))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7999e872b3343f2b6a21cee59a9b7b62922c1ea6773decd1532de0f3aea849d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
